##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
## Created by: RainbowSecret
## Microsoft Research
## yuyua@microsoft.com
## Copyright (c) 2018
##
## This source code is licensed under the MIT-style license found in the
## LICENSE file in the root directory of this source tree
##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

import os
import pdb
import torch
import torch.nn as nn
import torch.nn.functional as F

from lib.models.backbones.backbone_selector import BackboneSelector
from lib.models.tools.module_helper import ModuleHelper
from lib.models.modules.projection import ProjectionHead, PriorHead
from lib.utils.tools.logger import Logger as Log
from lib.models.modules.hanet_attention import HANet_Conv


class HRNet_W48(nn.Module):
    """
    deep high-resolution representation learning for human pose estimation, CVPR2019
    """

    def __init__(self, configer):
        super(HRNet_W48, self).__init__()
        self.configer = configer
        self.num_classes = self.configer.get('data', 'num_classes')
        self.backbone = BackboneSelector(configer).get_backbone()

        # extra added layers
        in_channels = 720  # 48 + 96 + 192 + 384
        self.cls_head = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(in_channels, bn_type=self.configer.get('network', 'bn_type')),
            nn.Dropout2d(0.10),
            nn.Conv2d(in_channels, self.num_classes, kernel_size=1, stride=1, padding=0, bias=False)
        )

    def forward(self, x_):
        x = self.backbone(x_)
        _, _, h, w = x[0].size()

        feat1 = x[0]
        feat2 = F.interpolate(x[1], size=(h, w), mode="bilinear", align_corners=True)
        feat3 = F.interpolate(x[2], size=(h, w), mode="bilinear", align_corners=True)
        feat4 = F.interpolate(x[3], size=(h, w), mode="bilinear", align_corners=True)

        feats = torch.cat([feat1, feat2, feat3, feat4], 1)
        out = self.cls_head(feats)
        out = F.interpolate(out, size=(x_.size(2), x_.size(3)), mode="bilinear", align_corners=True)
        return out

class ConvBNReLU(nn.Module):
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size,
                 padding='same',
                 **kwargs):
        super().__init__()

        self._conv = nn.Conv2d(
            in_channels, out_channels, kernel_size, padding=padding, **kwargs).cuda()
        self._batch_norm = nn.BatchNorm2d(out_channels).cuda()
        # self._batch_norm = nn.SyncBatchNorm(out_channels)
        self._relu = nn.ReLU()

    def forward(self, x):
        x = self._conv(x)
        x = self._batch_norm(x)
        x = self._relu(x)
        return x
class AggregationModule(nn.Module):
    """Aggregation Module"""

    def __init__(
            self,
            in_channels,
            out_channels,
            kernel_size,
    ):
        super(AggregationModule, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        padding = kernel_size // 2

        self.reduce_conv = ConvBNReLU(
            in_channels,
            out_channels,
            kernel_size=3,
            stride=1,
            padding=1,
        )

        self.t1 = nn.Conv2d(
            out_channels,
            out_channels,
            kernel_size=(kernel_size, 1),
            padding=(padding, 0),
            groups=out_channels,
        ).cuda()
        self.t2 = nn.Conv2d(
            out_channels,
            out_channels,
            kernel_size=(1, kernel_size),
            padding=(0, padding),
            groups=out_channels,
        ).cuda()

        self.p1 = nn.Conv2d(
            out_channels,
            out_channels,
            kernel_size=(1, kernel_size),
            padding=(0, padding),
            groups=out_channels,
        ).cuda()
        self.p2 = nn.Conv2d(
            out_channels,
            out_channels,
            kernel_size=(kernel_size, 1),
            padding=(padding, 0),
            groups=out_channels,
        ).cuda()
        self.norm = nn.BatchNorm2d(out_channels).cuda()
        self.relu = nn.ReLU()

    def forward(self, x):
        """Forward function."""
        x = self.reduce_conv(x)
        x1 = self.t1(x)
        x1 = self.t2(x1)

        x2 = self.p1(x)
        x2 = self.p2(x2)

        out = self.relu(self.norm(x1 + x2))
        return out

class PyramidPooling(nn.Module):
    """
    Reference:
        Zhao, Hengshuang, et al. *"Pyramid scene parsing network."*
    """
    def __init__(self, in_channels, norm_layer, up_kwargs):
        super(PyramidPooling, self).__init__()
        self.pool1 = nn.AdaptiveAvgPool2d(1)
        self.pool2 = nn.AdaptiveAvgPool2d(2)
        self.pool3 = nn.AdaptiveAvgPool2d(3)
        self.pool4 = nn.AdaptiveAvgPool2d(6)

        inter_channels = int(in_channels/4)
        self.conv1_1 = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 1, bias=False),
                                     norm_layer(inter_channels),
                                     nn.ReLU(True))
        self.conv2_0 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),
                                     norm_layer(inter_channels))
        self.conv2_1 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),
                                     norm_layer(inter_channels))
        self.conv2_2 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),
                                     norm_layer(inter_channels))
        self.conv2_5 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),
                                     norm_layer(inter_channels),
                                     nn.ReLU(True))
        # self.conv1 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False),
        #                         norm_layer(out_channels),
        #                         nn.ReLU(True))
        # self.conv2 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False),
        #                         norm_layer(out_channels),
        #                         nn.ReLU(True))
        # self.conv3 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False),
        #                         norm_layer(out_channels),
        #                         nn.ReLU(True))
        # self.conv4 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False),
        #                         norm_layer(out_channels),
        #                         nn.ReLU(True))
        # bilinear interpolate options
        self._up_kwargs = up_kwargs

    def forward(self, x):
        _, _, h, w = x.size()
        x1 = self.conv1_1(x)
        x2_1 = self.conv2_0(x1)
        x2_2 = F.interpolate(self.conv2_1(self.pool1(x1)), (h, w), **self._up_kwargs)
        x2_3 = F.interpolate(self.conv2_2(self.pool2(x1)), (h, w), **self._up_kwargs)
        x_out = self.conv2_5(F.relu_(x2_1 + x2_2 + x2_3))
        # feat1 = F.interpolate(self.conv1(self.pool1(x)), (h, w), **self._up_kwargs)
        # feat2 = F.interpolate(self.conv2(self.pool2(x)), (h, w), **self._up_kwargs)
        # feat3 = F.interpolate(self.conv3(self.pool3(x)), (h, w), **self._up_kwargs)
        # feat4 = F.interpolate(self.conv4(self.pool4(x)), (h, w), **self._up_kwargs)
        return x_out

class _BoundaryRefineModule(nn.Module):
    def __init__(self, dim):
        super(_BoundaryRefineModule, self).__init__()
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)

    def forward(self, x):
        residual = self.conv1(x)
        residual = self.relu(residual)
        residual = self.conv2(residual)
        out = x + residual
        return out

class ObjectAttentionBlock2D(nn.Module):
    '''
    The basic implementation for object context block
    Input:
        N X C X H X W
    Parameters:
        in_channels       : the dimension of the input feature map
        key_channels      : the dimension after the key/query transform
        scale             : choose the scale to downsample the input feature maps (save memory cost)
        bn_type           : specify the bn type
    Return:
        N X C X H X W
    '''
    def __init__(self,
                 in_channels,
                 key_channels,
                 scale=1,
                 bn_type=None):
        super(ObjectAttentionBlock2D, self).__init__()
        self.scale = scale
        self.in_channels = in_channels
        self.key_channels = key_channels
        self.pool = nn.MaxPool2d(kernel_size=(scale, scale))
        self.f_pixel = nn.Sequential(
            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,
                kernel_size=1, stride=1, padding=0, bias=False),
            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),
            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,
                kernel_size=1, stride=1, padding=0, bias=False),
            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),
        )
        self.f_object = nn.Sequential(
            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,
                kernel_size=1, stride=1, padding=0, bias=False),
            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),
            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,
                kernel_size=1, stride=1, padding=0, bias=False),
            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),
        )
        self.f_down = nn.Sequential(
            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,
                kernel_size=1, stride=1, padding=0, bias=False),
            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),
        )
        self.f_up = nn.Sequential(
            nn.Conv2d(in_channels=self.key_channels, out_channels=self.in_channels,
                kernel_size=1, stride=1, padding=0, bias=False),
            ModuleHelper.BNReLU(self.in_channels, bn_type=bn_type),
        )

    def forward(self, x, proxy):
        batch_size, h, w = x.size(0), x.size(2), x.size(3)
        if self.scale > 1:
            x = self.pool(x)

        query = self.f_pixel(x).view(batch_size, self.key_channels, -1)
        query = query.permute(0, 2, 1)
        key = self.f_object(proxy).view(batch_size, self.key_channels, -1)
        value = self.f_down(proxy).view(batch_size, self.key_channels, -1)
        value = value.permute(0, 2, 1)

        sim_map = torch.matmul(query, key)
        sim_map = (self.key_channels**-.5) * sim_map
        sim_map = F.softmax(sim_map, dim=-1)

        # add bg context ...
        context = torch.matmul(sim_map, value)
        context = context.permute(0, 2, 1).contiguous()
        context = context.view(batch_size, self.key_channels, *x.size()[2:])
        context = self.f_up(context)
        if self.scale > 1:
            context = F.interpolate(input=context, size=(h, w), mode='bilinear', align_corners=True)

        return context
class SpatialOCR_Module(nn.Module):
    """
    Implementation of the OCR module:
    We aggregate the global object representation to update the representation for each pixel.
    """
    def __init__(self,
                 in_channels,
                 key_channels,
                 out_channels,
                 scale=1,
                 dropout=0.1,
                 bn_type=None):
        super(SpatialOCR_Module, self).__init__()
        self.object_context_block = ObjectAttentionBlock2D(in_channels,
                                                           key_channels,
                                                           scale,
                                                           bn_type)
        _in_channels = 2 * in_channels

        self.conv_bn_dropout = nn.Sequential(
            nn.Conv2d(_in_channels, out_channels, kernel_size=1, padding=0, bias=False),
            ModuleHelper.BNReLU(out_channels, bn_type=bn_type),
            nn.Dropout2d(dropout)
        )

    def forward(self, feats, proxy_feats):
        context = self.object_context_block(feats, proxy_feats)

        output = self.conv_bn_dropout(torch.cat([context, feats], 1))

        return output
class HRNet_W48_CONTRAST(nn.Module):
    """
    deep high-resolution representation learning for human pose estimation, CVPR2019
    """

    def __init__(self, configer):
        super(HRNet_W48_CONTRAST, self).__init__()
        self.configer = configer
        self.num_classes = self.configer.get('data', 'num_classes')
        self.backbone = BackboneSelector(configer).get_backbone()
        self.proj_dim = self.configer.get('contrast', 'proj_dim')
        # self.prior_size = torch.tensor([proir_size, proir_size])

        # extra added layers
        in_channels = 720  # 48 + 96 + 192 + 384
        channels = 512
        self.cls_head = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(in_channels, bn_type=self.configer.get('network', 'bn_type')),
            nn.Dropout2d(0.10),
            nn.Conv2d(in_channels, self.num_classes, kernel_size=1, stride=1, padding=0, bias=False)
        )
        self.cls_out_head = nn.Sequential(
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(512, bn_type=self.configer.get('network', 'bn_type')),
            nn.Dropout2d(0.10),
            nn.Conv2d(512, self.num_classes, kernel_size=1, stride=1, padding=0, bias=False)
        )

        self.proj_head = ProjectionHead(dim_in=in_channels, proj_dim=self.proj_dim)
        # self.proj_head_ = ProjectionHead(dim_in=256, proj_dim=self.proj_dim)
        self.aggregation = AggregationModule(384, 512, 11)
        up_kwargs = {'mode': 'bilinear', 'align_corners': True}
        self.psp = PyramidPooling(384, nn.BatchNorm2d, up_kwargs)

        self.conv3 = nn.Sequential(nn.Conv2d(608, 512, 1, stride=2, bias=False),
                                   nn.BatchNorm2d(512))
        self.conv_feats = nn.Sequential(nn.Conv2d(512, 512, 1, stride=2, bias=False),
                                   nn.BatchNorm2d(512))
        self.brm = _BoundaryRefineModule(256)
        # self.aggregation2 = AggregationModule(720, 256, 11)
        self.prior_head =PriorHead()

        self.aux_head = nn.Sequential(
            nn.Conv2d(in_channels, channels,
                      kernel_size=1, stride=2, padding=0),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, self.num_classes,
                      kernel_size=1, stride=1, padding=0, bias=True)
        )

        self.intra_conv = ConvBNReLU(
            512, 256, 1, padding=0, stride=1)
        self.inter_conv = ConvBNReLU(
            512,
            256,
            1,
            padding=0,
            stride=1,
        )
        self.bottleneck = ConvBNReLU(
            1744,
            512,
            3,
            padding=1,
        )
        # conv_channel = 512
        # self.conv_head = nn.Sequential(
        #     nn.Conv2d(conv_channel, int(conv_channel/2), kernel_size=1, stride=1, padding=0),
        # )

        self.ocr_distri_head = SpatialOCR_Module(in_channels=512,
                                                 key_channels=256,
                                                 out_channels=512,
                                                 scale=1,
                                                 dropout=0.05,
                                                 bn_type=self.configer.get('network', 'bn_type')
                                                 )
        self.conv3x3_ocr = nn.Sequential(
            nn.Conv2d(in_channels, 512,
                      kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
        )
    def forward(self, x_, with_embed=False, is_eval=False):
        x = self.backbone(x_)
        batch_size, channel, h, w = x[0].size()

        feat1 = x[0]
        feat2 = F.interpolate(x[1], size=(h, w), mode="bilinear", align_corners=True)
        feat3 = F.interpolate(x[2], size=(h, w), mode="bilinear", align_corners=True)
        feat4 = F.interpolate(x[3], size=(h, w), mode="bilinear", align_corners=True)

        feats = torch.cat([feat1, feat2, feat3, feat4], 1)
        _, channel, height, width = feat4.size()

        # x_ex = self.backbone(x_ex_)
        # batch_size, channel, h, w = x_ex[0].size()

        # feat1_ex = x_ex[0]
        # feat2_ex = F.interpolate(x_ex[1], size=(h, w), mode="bilinear", align_corners=True)
        # feat3_ex = F.interpolate(x_ex[2], size=(h, w), mode="bilinear", align_corners=True)
        # feat4_ex = F.interpolate(x_ex[3], size=(h, w), mode="bilinear", align_corners=True)

        # feats_ex = torch.cat([feat1_ex, feat2_ex, feat3_ex, feat4_ex], 1)
        # _ex, channel_ex, height_ex, width_ex = feat4_ex.size()

        # self.prior_size = int(height*width/16)
        # channel = int(channel)
        # self.prior_conv = nn.Sequential(
        #     nn.Conv2d(
        #         256,
        #         self.prior_size,
        #         1,
        #         stride=4,
        #         padding=0,
        #         groups=1), nn.BatchNorm2d(self.prior_size)).cuda()
        # self.aggregation = AggregationModule(channel, 256, 11)


        value1 = self.aggregation(feat4) #aggregation module
        value2 = self.psp(feat4) #ppm module
        value = self.conv3(torch.cat([value1, value2], dim=1))
        # value = self.brm(value)
        # feat_value = self.aggregation2(feats)

        # context_prior_map = context_prior_map.reshape(
        #     (batch_size, self.prior_size, -1))
        # # context_prior_map = torch.permute(context_prior_map, (0, 2, 1))
        # context_prior_map = context_prior_map.permute(0, 2, 1)
        # context_prior_map = F.sigmoid(context_prior_map)

        out = self.cls_head(feats)

        # emb = self.proj_head(feats)

        ob_region = self.aux_head(feats)  # object region with loss
        probs = ob_region.view(batch_size, ob_region.size(1), -1)
        probs = F.softmax(probs, dim=2)    # object region possibility

        pixel_feats = self.conv3x3_ocr(feats)
        pixel_feats =pixel_feats.view(batch_size, pixel_feats.size(1), -1)
        pixel_feats = pixel_feats.permute(0, 2, 1)
        pixel_feats_probs = torch.matmul(probs, pixel_feats).permute(0, 2, 1).unsqueeze(3) #object region representation

        context_prior_map = self.prior_head(value)
        inter_context_prior_map = 1 - context_prior_map

        value = self.conv_feats(value)
        # value = self.conv_feats(value)
        value = value.reshape((batch_size, 512, -1))

        value = torch.transpose(value, 2, 1)

        intra_context = torch.bmm(context_prior_map, value)

        intra_context = (intra_context / intra_context.size(1)).permute(0, 2, 1)
        intra_context = intra_context.reshape((batch_size, -1, int(h/4), int(w/4)))
        intra_context = F.interpolate(intra_context, (h, w), mode='bilinear', align_corners=True)
        # intra_context = intra_context.reshape((batch_size, 512, -1)).permute(0, 2, 1)

        # intra_context = intra_context.transpose(2, 1)
        # intra_context = torch.matmul(probs, intra_context).permute(0, 2, 1).unsqueeze(3)# intra object region representation
        # pixel_feats = self.conv3x3_ocr(feats)
        intra_out_feats = self.ocr_distri_head(intra_context, pixel_feats_probs)


        # intra_context = intra_context.reshape((batch_size, 256,
        #                                        int(h/2),
        #                                        int(w/2)))
        # intra_context = F.interpolate(intra_context, (h, w), mode='bilinear', align_corners=True)
        # intra_context = self.intra_conv(intra_context)
        # intra_context_ = self.proj_head_(intra_context)

        inter_context = torch.bmm(inter_context_prior_map, value)
        inter_context = (inter_context / inter_context.size(1)).permute(0, 2, 1)
        inter_context = inter_context.reshape((batch_size, -1, int(h / 4), int(w / 4)))
        inter_context = F.interpolate(inter_context, (h, w), mode='bilinear', align_corners=True)
        inter_out_feats = self.ocr_distri_head(inter_context, pixel_feats_probs)
        # inter_context = inter_context / int(h * w / 16)
        # inter_context = inter_context.transpose(2, 1)
        # inter_context = inter_context.reshape((batch_size, 256,
        #                                        int(h / 2),
        #                                        int(w / 2)))
        # inter_context = F.interpolate(inter_context, (h, w), mode='bilinear', align_corners=True)
        # inter_context = self.inter_conv(inter_context)
        # inter_context_ = self.proj_head_(inter_context)


        cp_outs = torch.cat([feats, intra_out_feats, inter_out_feats], axis=1)
        output = self.bottleneck(cp_outs)

        output = self.cls_out_head(output)
        return { 'seg': output, 'seg_out': out, 'prior': context_prior_map, 'ob_region': ob_region} #'embed': emb} 'seg': out,


class HRNet_W48_OCR_CONTRAST(nn.Module):
    def __init__(self, configer):
        super(HRNet_W48_OCR_CONTRAST, self).__init__()
        self.configer = configer
        self.num_classes = self.configer.get('data', 'num_classes')
        self.backbone = BackboneSelector(configer).get_backbone()
        self.proj_dim = self.configer.get('contrast', 'proj_dim')

        in_channels = 720
        self.conv3x3 = nn.Sequential(
            nn.Conv2d(in_channels, 512, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(512, bn_type=self.configer.get('network', 'bn_type')),
        )
        from lib.models.modules.spatial_ocr_block import SpatialGather_Module
        self.ocr_gather_head = SpatialGather_Module(self.num_classes)
        from lib.models.modules.spatial_ocr_block import SpatialOCR_Module
        self.ocr_distri_head = SpatialOCR_Module(in_channels=512,
                                                 key_channels=256,
                                                 out_channels=512,
                                                 scale=1,
                                                 dropout=0.05,
                                                 bn_type=self.configer.get('network', 'bn_type'))
        self.cls_head = nn.Conv2d(512, self.num_classes, kernel_size=1, stride=1, padding=0, bias=True)
        self.aux_head = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(in_channels, bn_type=self.configer.get('network', 'bn_type')),
            nn.Conv2d(in_channels, self.num_classes, kernel_size=1, stride=1, padding=0, bias=True)
        )

        self.proj_head = ProjectionHead(dim_in=in_channels, proj_dim=self.proj_dim)

        self.aggregation = AggregationModule(384, 512, 11)
        self.prior_head = PriorHead()
        up_kwargs = {'mode': 'bilinear', 'align_corners': True}
        self.psp = PyramidPooling(384, nn.BatchNorm2d, up_kwargs)
        self.conv3 = nn.Sequential(nn.Conv2d(608, 512, 1, stride=1, bias=False),
                                   nn.BatchNorm2d(512))
        # self.conv4 = nn.Sequential(nn.Conv2d(1024, 512, 1, stride=1, bias=False),
        #                            nn.BatchNorm2d(512))
        self.bottleneck = ConvBNReLU(
            1024,
            512,
            3,
            padding=1,
        )
        self.intra_conv = ConvBNReLU(
            512, 512, 1, padding=0, stride=1)
        self.inter_conv = ConvBNReLU(
            512,
            512,
            1,
            padding=0,
            stride=1,
        )

    def forward(self, x_, with_embed=False, is_eval=False):
        x = self.backbone(x_)
        batch_size, _, h, w = x[0].size()

        feat1 = x[0]
        feat2 = F.interpolate(x[1], size=(h, w), mode="bilinear", align_corners=True)
        feat3 = F.interpolate(x[2], size=(h, w), mode="bilinear", align_corners=True)
        feat4 = F.interpolate(x[3], size=(h, w), mode="bilinear", align_corners=True)

        # f3 = x[2]
        # f4 = x[3]
        f4 = F.interpolate(x[3], size=(x[2].size(2), x[2].size(3)), mode="bilinear", align_corners=True)
        value = self.aggregation(f4)  # aggregation module
        # value2 = self.psp(f4)  # ppm module
        # value = self.conv3(torch.cat([value1, value2], dim=1))
        context_prior_map = self.prior_head(value)
        inter_context_prior_map = 1 - context_prior_map
        value = value.reshape((batch_size, 512, -1))
        value = torch.transpose(value, 2, 1)

        intra_context = torch.bmm(context_prior_map, value)
        intra_context = intra_context / intra_context.size(1)
        intra_context = torch.transpose(intra_context, 2, 1)
        intra_context = intra_context.reshape((batch_size, -1, int(h / 4), int(w / 4)))
        intra_context = F.interpolate(intra_context, (h, w), mode='bilinear', align_corners=True)
        intra_context = self.intra_conv(intra_context)

        # inter_context = torch.bmm(inter_context_prior_map, value)
        # inter_context = inter_context / inter_context.size(1)
        # inter_context = torch.transpose(inter_context, 2, 1)
        # inter_context = inter_context.reshape((batch_size, -1, int(h / 4), int(w / 4)))
        # inter_context = F.interpolate(inter_context, (h, w), mode='bilinear', align_corners=True)
        # inter_context = self.inter_conv(inter_context)

        feats = torch.cat([feat1, feat2, feat3, feat4], 1)
        # value = self.aggregation(feat4)
        out_aux = self.aux_head(feats)

        # emb = self.proj_head(feats)

        feats = self.conv3x3(feats)

        context = self.ocr_gather_head(feats, out_aux)
        feats = self.ocr_distri_head(feats, context)

        intra_feats = self.ocr_distri_head(intra_context, context)
        # inter_feats = self.ocr_distri_head(inter_context, context)
        cp_outs = torch.cat([feats, intra_feats], axis=1)
        output = self.bottleneck(cp_outs)
        out = self.cls_head(output)

        return {'seg': out, 'seg_aux': out_aux, 'prior': context_prior_map}#, 'embed': emb}#


class HRNet_W48_MEM(nn.Module):
    def __init__(self, configer, dim=256, m=0.999, with_masked_ppm=False):
        super(HRNet_W48_MEM, self).__init__()
        self.configer = configer
        self.m = m
        self.r = self.configer.get('contrast', 'memory_size')
        self.with_masked_ppm = with_masked_ppm

        num_classes = self.configer.get('data', 'num_classes')

        self.encoder_q = HRNet_W48_CONTRAST(configer)

        self.register_buffer("segment_queue", torch.randn(num_classes, self.r, dim))
        self.segment_queue = nn.functional.normalize(self.segment_queue, p=2, dim=2)
        self.register_buffer("segment_queue_ptr", torch.zeros(num_classes, dtype=torch.long))

        self.register_buffer("pixel_queue", torch.randn(num_classes, self.r, dim))
        self.pixel_queue = nn.functional.normalize(self.pixel_queue, p=2, dim=2)
        self.register_buffer("pixel_queue_ptr", torch.zeros(num_classes, dtype=torch.long))

    @torch.no_grad()
    def _momentum_update_key_encoder(self):
        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):
            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)

    def forward(self, im_q, lb_q=None, with_embed=True, is_eval=False):
        if is_eval is True or lb_q is None:
            ret = self.encoder_q(im_q, with_embed=with_embed)
            return ret

        ret = self.encoder_q(im_q)

        q = ret['embed']
        out = ret['seg']
        # prior = ret['prior']
        # feats = ret['feats']

        # return {'seg': out, 'embed': q, 'prior': prior, 'key': q.detach(), 'lb_key': lb_q.detach()}

        return {'seg': out, 'embed': q, 'key': q.detach(), 'lb_key': lb_q.detach()}


class HRNet_W48_OCR(nn.Module):
    def __init__(self, configer):
        super(HRNet_W48_OCR, self).__init__()
        self.configer = configer
        self.num_classes = self.configer.get('data', 'num_classes')
        self.backbone = BackboneSelector(configer).get_backbone()

        in_channels = 720
        self.conv3x3 = nn.Sequential(
            nn.Conv2d(in_channels, 512, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(512, bn_type=self.configer.get('network', 'bn_type')),
        )
        from lib.models.modules.spatial_ocr_block import SpatialGather_Module
        self.ocr_gather_head = SpatialGather_Module(self.num_classes)
        from lib.models.modules.spatial_ocr_block import SpatialOCR_Module
        self.ocr_distri_head = SpatialOCR_Module(in_channels=512,
                                                 key_channels=256,
                                                 out_channels=512,
                                                 scale=1,
                                                 dropout=0.05,
                                                 bn_type=self.configer.get('network', 'bn_type'))
        self.cls_head = nn.Conv2d(512, self.num_classes, kernel_size=1, stride=1, padding=0, bias=True)
        self.aux_head = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(in_channels, bn_type=self.configer.get('network', 'bn_type')),
            nn.Conv2d(in_channels, self.num_classes, kernel_size=1, stride=1, padding=0, bias=True)
        )

    def forward(self, x_):
        x = self.backbone(x_)
        _, _, h, w = x[0].size()

        feat1 = x[0]
        feat2 = F.interpolate(x[1], size=(h, w), mode="bilinear", align_corners=True)
        feat3 = F.interpolate(x[2], size=(h, w), mode="bilinear", align_corners=True)
        feat4 = F.interpolate(x[3], size=(h, w), mode="bilinear", align_corners=True)

        feats = torch.cat([feat1, feat2, feat3, feat4], 1)
        out_aux = self.aux_head(feats)

        feats = self.conv3x3(feats)

        context = self.ocr_gather_head(feats, out_aux)
        feats = self.ocr_distri_head(feats, context)

        out = self.cls_head(feats)

        out_aux = F.interpolate(out_aux, size=(x_.size(2), x_.size(3)), mode="bilinear", align_corners=True)
        out = F.interpolate(out, size=(x_.size(2), x_.size(3)), mode="bilinear", align_corners=True)
        return out_aux, out


class HRNet_W48_OCR_B(nn.Module):
    """
    Considering that the 3x3 convolution on the 4x resolution feature map is expensive,
    we can decrease the intermediate channels from 512 to 256 w/o performance loss.
    """

    def __init__(self, configer):
        super(HRNet_W48_OCR_B, self).__init__()
        self.configer = configer
        self.num_classes = self.configer.get('data', 'num_classes')
        self.backbone = BackboneSelector(configer).get_backbone()

        in_channels = 720  # 48 + 96 + 192 + 384
        self.conv3x3 = nn.Sequential(
            nn.Conv2d(in_channels, 256, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(256, bn_type=self.configer.get('network', 'bn_type')),
        )
        from lib.models.modules.spatial_ocr_block import SpatialGather_Module
        self.ocr_gather_head = SpatialGather_Module(self.num_classes)
        from lib.models.modules.spatial_ocr_block import SpatialOCR_Module
        self.ocr_distri_head = SpatialOCR_Module(in_channels=256,
                                                 key_channels=128,
                                                 out_channels=256,
                                                 scale=1,
                                                 dropout=0.05,
                                                 bn_type=self.configer.get('network', 'bn_type'))

        self.cls_head = nn.Conv2d(256, self.num_classes, kernel_size=1, stride=1, padding=0, bias=True)
        self.aux_head = nn.Sequential(
            nn.Conv2d(in_channels, 256, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(256, bn_type=self.configer.get('network', 'bn_type')),
            nn.Conv2d(256, self.num_classes, kernel_size=1, stride=1, padding=0, bias=True)
        )

    def forward(self, x_):
        x = self.backbone(x_)
        _, _, h, w = x[0].size()

        feat1 = x[0]
        feat2 = F.interpolate(x[1], size=(h, w), mode="bilinear", align_corners=True)
        feat3 = F.interpolate(x[2], size=(h, w), mode="bilinear", align_corners=True)
        feat4 = F.interpolate(x[3], size=(h, w), mode="bilinear", align_corners=True)

        feats = torch.cat([feat1, feat2, feat3, feat4], 1)
        out_aux = self.aux_head(feats)

        feats = self.conv3x3(feats)

        context = self.ocr_gather_head(feats, out_aux)
        feats = self.ocr_distri_head(feats, context)

        out = self.cls_head(feats)

        out_aux = F.interpolate(out_aux, size=(x_.size(2), x_.size(3)), mode="bilinear", align_corners=True)
        out = F.interpolate(out, size=(x_.size(2), x_.size(3)), mode="bilinear", align_corners=True)
        return out_aux, out


class HRNet_W48_OCR_B_HA(nn.Module):
    """
    Considering that the 3x3 convolution on the 4x resolution feature map is expensive,
    we can decrease the intermediate channels from 512 to 256 w/o performance loss.
    """

    def __init__(self, configer):
        super(HRNet_W48_OCR_B_HA, self).__init__()
        self.configer = configer
        self.num_classes = self.configer.get('data', 'num_classes')
        self.backbone = BackboneSelector(configer).get_backbone()

        in_channels = 720  # 48 + 96 + 192 + 384
        self.conv3x3 = nn.Sequential(
            nn.Conv2d(in_channels, 256, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(256, bn_type=self.configer.get('network', 'bn_type')),
        )
        from lib.models.modules.spatial_ocr_block import SpatialGather_Module
        self.ocr_gather_head = SpatialGather_Module(self.num_classes)
        from lib.models.modules.spatial_ocr_block import SpatialOCR_Module
        self.ocr_distri_head = SpatialOCR_Module(in_channels=256,
                                                 key_channels=128,
                                                 out_channels=256,
                                                 scale=1,
                                                 dropout=0.05,
                                                 bn_type=self.configer.get('network', 'bn_type'))
        self.cls_head = nn.Conv2d(256, self.num_classes, kernel_size=1, stride=1, padding=0, bias=True)
        self.aux_head = nn.Sequential(
            nn.Conv2d(in_channels, 256, kernel_size=3, stride=1, padding=1),
            ModuleHelper.BNReLU(256, bn_type=self.configer.get('network', 'bn_type')),
            nn.Conv2d(256, self.num_classes, kernel_size=1, stride=1, padding=0, bias=True)
        )

        self.ha1 = HANet_Conv(384, 384, bn_type=self.configer.get('network', 'bn_type'))
        self.ha2 = HANet_Conv(192, 192, bn_type=self.configer.get('network', 'bn_type'))
        self.ha3 = HANet_Conv(96, 96, bn_type=self.configer.get('network', 'bn_type'))
        self.ha4 = HANet_Conv(48, 48, bn_type=self.configer.get('network', 'bn_type'))

    def forward(self, x_):
        x = self.backbone(x_)
        _, _, h, w = x[0].size()

        x[0] = x[0] + self.ha1(x[0])
        x[1] = x[1] + self.ha1(x[1])
        x[2] = x[2] + self.ha1(x[2])
        x[3] = x[3] + self.ha1(x[3])

        feat1 = x[0]
        feat2 = F.interpolate(x[1], size=(h, w), mode="bilinear", align_corners=True)
        feat3 = F.interpolate(x[2], size=(h, w), mode="bilinear", align_corners=True)
        feat4 = F.interpolate(x[3], size=(h, w), mode="bilinear", align_corners=True)

        feats = torch.cat([feat1, feat2, feat3, feat4], 1)
        out_aux = self.aux_head(feats)

        feats = self.conv3x3(feats)

        context = self.ocr_gather_head(feats, out_aux)
        feats = self.ocr_distri_head(feats, context)

        out = self.cls_head(feats)

        out_aux = F.interpolate(out_aux, size=(x_.size(2), x_.size(3)), mode="bilinear", align_corners=True)
        out = F.interpolate(out, size=(x_.size(2), x_.size(3)), mode="bilinear", align_corners=True)
        return out_aux, out
